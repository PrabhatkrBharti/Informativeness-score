review_id,review,sentence count,word count,avg. sentence length,avg. word length,vocab length,hedge words,non-hedge words,hedge ratio,sections covered (out of 14),aspects covered (out of 9),section distribution,aspect distribution,section score,aspect score,informativeness score,hedge_score,vader compound sentiment,noun_count,adj_count,verb_count,adverb_count,new info score
B1EA-M-0Z__R3,"This paper presents a new covariance function for Gaussian processes (GPs) that is equivalent to a Bayesian deep neural network with a Gaussian prior on the weights and an infinite width. As a result, exact Bayesian inference with a deep neural network can be solved with the standard GP machinery.  Pros: The result highlights an interesting relationship between deep nets and Gaussian processes. (Although I am unsure about how much of the kernel design had already appeared outside of the GP literature.)  The paper is clear and very well written. The analysis of the phases in the hyperparameter space is interesting and insightful. On the other hand, one of the great assets of GPs is the powerful way to tune their hyperparameters via maximisation of the marginal likelihood but the authors have left this for future work! Cons: Although the computational complexity of computing the covariance matrix is given, no actual computational times are reported in the article. I suggest using the same axis limits for all subplots in Fig 3.",9,171,24.4285714285714,5.16564417177914,107,0,171,0,8,4,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 2, 'EXP': 2, 'RES': 1, 'TNF': 1, 'ANA': 1, 'FWK': 1, 'OAL': 1, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 1, 'IMP': 1, 'CMP': 0, 'PNF': 0, 'REC': 0, 'EMP': 3, 'SUB': 0, 'CLA': 1}",0.571764959334065,0.44568835931413,0.328460424069274,0.023121387283237,0.8243,50,30,21,3,0.320953087859084
ByuP8yZRb__R2,"The below review addresses the first revision of the paper . The revised version does address my concerns. The fact that the paper does not come with substantial theoretical contributions/justification still stands out. The authors present a variant of the adversarial feature learning (AFL) approach by Edwards Storkey. AFL aims to find a data representation that allows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S. The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarial model predicting S is minimized.  The authors suggest the use of multiple adversarial models, which can be interpreted as using an ensemble model instead of a single model. The way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq. 2. ,[none],[none]] While there is no requirement to have a distribution here—a simple loss term is sufficient—the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary. Hence, lambda in Eq. 3 may need to be chosen differently depending on the adversarial model. Without tuning lambda for each method, the empirical experiments seem unfair. This may also explain why, for example, the baseline method with one adversary effectively fails for Opp-L. A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas. The area under this curve allows much better to compare the various methods. There are little theoretical contributions. Basically, instead of a single adversarial model—e.g., a single-layer NN or a multi-layer NN—the authors propose to train multiple adversarial models on different views of the data. An alternative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set. Though, there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary. Tuning the architecture of the single multi-layer NN adversary might be as good? In short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods. This renders the comparison unfair. Given that there is also no theoretical argument why an ensemble approach is expected to perform better, I recommend to reject the paper.",20,402,17.4782608695652,5.60582010582011,191,3,399,0.007518796992481,6,3,"{'ABS': 0, 'INT': 0, 'RWK': 5, 'PDI': 1, 'DAT': 0, 'MET': 11, 'EXP': 5, 'RES': 0, 'TNF': 0, 'ANA': 0, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 1}","{'APR': 0, 'NOV': 0, 'IMP': 0, 'CMP': 2, 'PNF': 0, 'REC': 1, 'EMP': 8, 'SUB': 0, 'CLA': 0}",0.431785503293697,0.337804197934424,0.221011413664662,0.032178217821782,-0.9484,119,53,72,14,0.215628048336042
H1meywxRW__R1,"The authors of this paper propose some extensions to the Dynamic Coattention Networks models presented last year at ICLR. First they modify the architecture of the answer selection model by adding an extra coattention layer to improve the capture of dependencies between question and answer descriptions. The other main modification is to train their DCN+ model using both cross entropy loss and F1 score (using RL supervision) in order to reward the system for making partial matching predictions. Empirical evaluations conducted on the SQuAD dataset indicates that this architecture achieves an improvement of at least 3%, both on F1 and exact match accuracy, over other comparable systems. An ablation study clearly shows the contribution of the deep coattention mechanism and mixed objective training on the model performance.  The paper is well written, ideas are presented clearly and the experiments section provide interesting insights such as the impact of RL on system training or the capability of the model to handle long questions and/or answers. It seems to me that this paper is a significant contribution to the field of question answering systems. ",7,182,22.75,5.39887640449438,114,1,181,0.005524861878453,6,6,"{'ABS': 0, 'INT': 1, 'RWK': 0, 'PDI': 0, 'DAT': 0, 'MET': 3, 'EXP': 2, 'RES': 1, 'TNF': 0, 'ANA': 1, 'FWK': 0, 'OAL': 2, 'BIB': 0, 'EXT': 0}","{'APR': 0, 'NOV': 0, 'IMP': 1, 'CMP': 1, 'PNF': 1, 'REC': 1, 'EMP': 2, 'SUB': 0, 'CLA': 1}",0.429203427398757,0.66728862410151,0.306033556743194,0.027173913043478,0.9633,66,20,25,3,0.299479421995722
